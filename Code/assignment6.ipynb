{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import spacy\n",
    "import pandas as pd\n",
    "from tqdm import tqdm \n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "from spacy.lang.en import English\n",
    "import random\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('BetaData.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALPHA = 0.5\n",
    "BETA = 0.5\n",
    "NUM_TOPICS = 40\n",
    "sp = spacy.load('en_core_web_sm')\n",
    "\n",
    "np.random.seed(100)\n",
    "random.seed(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_frequencies(data, max_docs = 10000):\n",
    "    freqs = Counter()\n",
    "    all_stopwords = sp.Defaults.stop_words\n",
    "    all_stopwords.add('okidoki')\n",
    "    all_stopwords.add('hey')\n",
    "    all_stopwords.add('yep')\n",
    "    all_stopwords.add('girl')\n",
    "    all_stopwords.add('na')\n",
    "    all_stopwords.add('nuh')\n",
    "    all_stopwords.add('null')\n",
    "    all_stopwords.add('yoh')\n",
    "    all_stopwords.add('okay')\n",
    "    all_stopwords.add('yup')\n",
    "    all_stopwords.add('eish')\n",
    "    all_stopwords.add('ai')\n",
    "    all_stopwords.add('sure')\n",
    "    all_stopwords.add('oh')\n",
    "    all_stopwords.add('hi')\n",
    "    all_stopwords.add('nope')\n",
    "    all_stopwords.add('awe')\n",
    "    all_stopwords.add('https')\n",
    "    all_stopwords.add('ah')\n",
    "    all_stopwords.add('heyo')\n",
    "    all_stopwords.add('whoop')\n",
    "    all_stopwords.add('yeah')\n",
    "    all_stopwords.add('gon')\n",
    "    all_stopwords.add('said')\n",
    "    all_stopwords.add('yes')\n",
    "    all_stopwords.add('know')\n",
    "    all_stopwords.add(\"enron\")\n",
    "    nr_tokens = 0\n",
    "\n",
    "    for doc in data[:max_docs]:\n",
    "        if isinstance(doc, str):\n",
    "            tokens = sp.tokenizer(doc)\n",
    "            for token in tokens:\n",
    "                token_text = token.text.lower()\n",
    "                if token_text not in all_stopwords and token.is_alpha:\n",
    "                    nr_tokens += 1\n",
    "                    freqs[token_text] += 1\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    return freqs\n",
    "\n",
    "def get_vocab(freqs, freq_threshold=3):\n",
    "    vocab = {}\n",
    "    vocab_idx_str = {}\n",
    "    vocab_idx = 0\n",
    "\n",
    "    for word in freqs:\n",
    "        if freqs[word] >= freq_threshold:\n",
    "            vocab[word]= vocab_idx\n",
    "            vocab_idx_str[vocab_idx] = word\n",
    "            vocab_idx += 1\n",
    "    return vocab, vocab_idx_str\n",
    "        \n",
    "\n",
    "\n",
    "def tokenize_dataset(data, vocab, max_docs= 10000):\n",
    "    nr_tokens = 0\n",
    "    nr_docs = 0\n",
    "    docs = []\n",
    "\n",
    "    for doc in data[:max_docs]:\n",
    "        tokens = sp.tokenizer(doc)\n",
    "\n",
    "        if len(tokens) > 1:\n",
    "            doc = []\n",
    "            for token in tokens:\n",
    "                token_text = token.text.lower()\n",
    "                if token_text in vocab:\n",
    "                    doc.append(token_text)\n",
    "                    nr_tokens += 1\n",
    "            nr_docs += 1\n",
    "            docs.append(doc)\n",
    "\n",
    "    print(f\"Number of text messages: {nr_docs}\")\n",
    "    print(f\"Number of tokens: {nr_tokens}\")\n",
    "\n",
    "    corpus = []\n",
    "    for doc in docs:\n",
    "        corpus_d = []\n",
    "\n",
    "        for token in doc:\n",
    "            corpus_d.append(vocab[token])\n",
    "\n",
    "        corpus.append(np.asarray(corpus_d))\n",
    "\n",
    "    return docs, corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of text messages: 1425\n",
      "Number of tokens: 1335\n",
      "Vocab size: 221\n"
     ]
    }
   ],
   "source": [
    "data = df['Column1'].astype(str).sample(frac=1).values\n",
    "freqs = generate_frequencies(data)\n",
    "vocab, vocab_idx_str = get_vocab(freqs)\n",
    "docs, corpus = tokenize_dataset(data, vocab)\n",
    "vocab_size = len(vocab)\n",
    "print(f\"Vocab size: {vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:01<00:00, 103.42it/s]\n"
     ]
    }
   ],
   "source": [
    "def LDA_Collapsed_Gibbs(corpus, num_iter=200):\n",
    "\n",
    "    Z = []\n",
    "    num_docs = len(corpus)\n",
    "\n",
    "    for _, doc in enumerate(corpus):\n",
    "        Zd = np.random.randint(low=0, high = NUM_TOPICS, size=(len(doc)))\n",
    "        Z.append(Zd)\n",
    "\n",
    "    ndk = np.zeros((num_docs, NUM_TOPICS))\n",
    "    for d in range(num_docs):\n",
    "        for k in range(NUM_TOPICS):\n",
    "            ndk[d, k] = np.sum(Z[d]==k)\n",
    "\n",
    "    nkw = np.zeros((NUM_TOPICS, vocab_size))\n",
    "    for doc_idx, doc in enumerate(corpus):\n",
    "        for i, word in enumerate(doc):\n",
    "            topic = Z[doc_idx][i]\n",
    "            nkw[topic, word] += 1\n",
    "\n",
    "    nk = np.sum(nkw, axis=1)\n",
    "    topic_list = [i for i in range(NUM_TOPICS)]\n",
    "\n",
    "    for _ in tqdm(range(num_iter)):\n",
    "        for doc_idx, doc in enumerate(corpus):\n",
    "            for i in range(len(doc)):\n",
    "                word = doc[i]\n",
    "                topic = Z[doc_idx][i]\n",
    "\n",
    "                ndk[doc_idx, topic] -= 1\n",
    "                nkw[topic, word] -= 1\n",
    "                nk[topic] -= 1\n",
    "\n",
    "                p_z = (ndk[doc_idx, :] + ALPHA * (nkw[:, word] + BETA)/ (nk[:] + BETA*vocab_size))\n",
    "                topic = random.choices(topic_list, weights=p_z, k=1)[0]\n",
    "\n",
    "                Z[doc_idx][i] = topic\n",
    "                ndk[doc_idx, topic] += 1\n",
    "                nkw[topic, word] += 1\n",
    "                nk[topic] += 1\n",
    "\n",
    "    return Z, ndk, nkw, nk\n",
    "\n",
    "Z, ndk, nkw, nk = LDA_Collapsed_Gibbs(corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0 most common words: \n",
      "way\n",
      "dumb\n",
      "coming\n",
      "time\n",
      "better\n",
      "wants\n",
      "nah\n",
      "delivery\n",
      "sending\n",
      "mind\n",
      "thinking\n",
      "trust\n",
      "study\n",
      "getting\n",
      "paid\n",
      "\n",
      "\n",
      "Topic 1 most common words: \n",
      "morning\n",
      "today\n",
      "time\n",
      "send\n",
      "happened\n",
      "better\n",
      "t\n",
      "paid\n",
      "worry\n",
      "wanted\n",
      "email\n",
      "tomorrow\n",
      "soon\n",
      "tight\n",
      "sleep\n",
      "\n",
      "\n",
      "Topic 2 most common words: \n",
      "meet\n",
      "page\n",
      "laundry\n",
      "chat\n",
      "vn\n",
      "listen\n",
      "class\n",
      "story\n",
      "week\n",
      "paid\n",
      "mind\n",
      "moment\n",
      "year\n",
      "crime\n",
      "missed\n",
      "\n",
      "\n",
      "Topic 3 most common words: \n",
      "thought\n",
      "kill\n",
      "send\n",
      "year\n",
      "tomorrow\n",
      "tell\n",
      "soon\n",
      "tonight\n",
      "weekend\n",
      "moving\n",
      "skipping\n",
      "pick\n",
      "told\n",
      "excited\n",
      "date\n",
      "\n",
      "\n",
      "Topic 4 most common words: \n",
      "year\n",
      "happy\n",
      "let\n",
      "going\n",
      "new\n",
      "hope\n",
      "friend\n",
      "delivery\n",
      "pick\n",
      "t\n",
      "try\n",
      "wrong\n",
      "gate\n",
      "happened\n",
      "desk\n",
      "\n",
      "\n",
      "Topic 5 most common words: \n",
      "outside\n",
      "saying\n",
      "wait\n",
      "right\n",
      "asked\n",
      "talk\n",
      "pay\n",
      "meeting\n",
      "leaving\n",
      "hope\n",
      "mind\n",
      "google\n",
      "started\n",
      "soon\n",
      "apartment\n",
      "\n",
      "\n",
      "Topic 6 most common words: \n",
      "sorry\n",
      "want\n",
      "work\n",
      "like\n",
      "coming\n",
      "told\n",
      "think\n",
      "forward\n",
      "thing\n",
      "change\n",
      "text\n",
      "outside\n",
      "talking\n",
      "got\n",
      "hope\n",
      "\n",
      "\n",
      "Topic 7 most common words: \n",
      "saw\n",
      "ready\n",
      "leave\n",
      "time\n",
      "let\n",
      "update\n",
      "things\n",
      "friend\n",
      "peanut\n",
      "soon\n",
      "sent\n",
      "lost\n",
      "got\n",
      "actually\n",
      "best\n",
      "\n",
      "\n",
      "Topic 8 most common words: \n",
      "home\n",
      "bring\n",
      "come\n",
      "working\n",
      "tonight\n",
      "delivery\n",
      "morning\n",
      "tomorrow\n",
      "moving\n",
      "message\n",
      "moment\n",
      "thank\n",
      "chat\n",
      "mean\n",
      "frame\n",
      "\n",
      "\n",
      "Topic 9 most common words: \n",
      "fine\n",
      "murder\n",
      "kea\n",
      "thing\n",
      "alright\n",
      "hatfield\n",
      "help\n",
      "change\n",
      "uber\n",
      "good\n",
      "hours\n",
      "wanna\n",
      "talk\n",
      "tomorrow\n",
      "friend\n",
      "\n",
      "\n",
      "Topic 10 most common words: \n",
      "mind\n",
      "feel\n",
      "asking\n",
      "let\n",
      "watching\n",
      "week\n",
      "day\n",
      "actually\n",
      "leave\n",
      "morning\n",
      "sending\n",
      "come\n",
      "room\n",
      "work\n",
      "study\n",
      "\n",
      "\n",
      "Topic 11 most common words: \n",
      "work\n",
      "mean\n",
      "maybe\n",
      "night\n",
      "long\n",
      "like\n",
      "find\n",
      "thank\n",
      "friends\n",
      "text\n",
      "interested\n",
      "hours\n",
      "way\n",
      "texting\n",
      "talk\n",
      "\n",
      "\n",
      "Topic 12 most common words: \n",
      "think\n",
      "night\n",
      "got\n",
      "hear\n",
      "sorry\n",
      "ready\n",
      "wrong\n",
      "talk\n",
      "wanna\n",
      "eh\n",
      "started\n",
      "kay\n",
      "better\n",
      "thing\n",
      "text\n",
      "\n",
      "\n",
      "Topic 13 most common words: \n",
      "voice\n",
      "ask\n",
      "missed\n",
      "kay\n",
      "try\n",
      "week\n",
      "come\n",
      "mom\n",
      "let\n",
      "meter\n",
      "ready\n",
      "straight\n",
      "lease\n",
      "tenant\n",
      "makes\n",
      "\n",
      "\n",
      "Topic 14 most common words: \n",
      "think\n",
      "got\n",
      "going\n",
      "buy\n",
      "time\n",
      "interested\n",
      "money\n",
      "today\n",
      "forward\n",
      "help\n",
      "cleaning\n",
      "meeting\n",
      "wanted\n",
      "guys\n",
      "wants\n",
      "\n",
      "\n",
      "Topic 15 most common words: \n",
      "coming\n",
      "ready\n",
      "let\n",
      "getting\n",
      "finished\n",
      "arrive\n",
      "come\n",
      "thank\n",
      "probably\n",
      "month\n",
      "morning\n",
      "tomorrow\n",
      "pay\n",
      "kea\n",
      "alright\n",
      "\n",
      "\n",
      "Topic 16 most common words: \n",
      "good\n",
      "going\n",
      "want\n",
      "meet\n",
      "let\n",
      "texting\n",
      "meeting\n",
      "morning\n",
      "late\n",
      "change\n",
      "need\n",
      "try\n",
      "office\n",
      "hope\n",
      "work\n",
      "\n",
      "\n",
      "Topic 17 most common words: \n",
      "help\n",
      "days\n",
      "open\n",
      "going\n",
      "today\n",
      "year\n",
      "partner\n",
      "leaving\n",
      "trust\n",
      "happy\n",
      "new\n",
      "took\n",
      "mind\n",
      "find\n",
      "missed\n",
      "\n",
      "\n",
      "Topic 18 most common words: \n",
      "carry\n",
      "going\n",
      "let\n",
      "group\n",
      "asking\n",
      "edited\n",
      "weekend\n",
      "missed\n",
      "voice\n",
      "hear\n",
      "place\n",
      "sweet\n",
      "home\n",
      "need\n",
      "use\n",
      "\n",
      "\n",
      "Topic 19 most common words: \n",
      "happened\n",
      "coming\n",
      "let\n",
      "kea\n",
      "email\n",
      "class\n",
      "chat\n",
      "change\n",
      "sold\n",
      "vn\n",
      "waiting\n",
      "outside\n",
      "send\n",
      "wants\n",
      "tonight\n",
      "\n",
      "\n",
      "Topic 20 most common words: \n",
      "day\n",
      "carry\n",
      "check\n",
      "need\n",
      "tired\n",
      "year\n",
      "skipping\n",
      "t\n",
      "hope\n",
      "work\n",
      "great\n",
      "actually\n",
      "meeting\n",
      "way\n",
      "love\n",
      "\n",
      "\n",
      "Topic 21 most common words: \n",
      "kay\n",
      "pay\n",
      "year\n",
      "playing\n",
      "missed\n",
      "understand\n",
      "want\n",
      "hours\n",
      "start\n",
      "try\n",
      "tonight\n",
      "uber\n",
      "going\n",
      "outside\n",
      "hope\n",
      "\n",
      "\n",
      "Topic 22 most common words: \n",
      "coming\n",
      "number\n",
      "guys\n",
      "gate\n",
      "right\n",
      "kea\n",
      "moving\n",
      "account\n",
      "talking\n",
      "delivery\n",
      "mean\n",
      "way\n",
      "tired\n",
      "tonight\n",
      "work\n",
      "\n",
      "\n",
      "Topic 23 most common words: \n",
      "wanna\n",
      "alright\n",
      "coming\n",
      "time\n",
      "bed\n",
      "stuff\n",
      "thank\n",
      "meeting\n",
      "movie\n",
      "going\n",
      "sorry\n",
      "cheat\n",
      "listen\n",
      "wanted\n",
      "waiting\n",
      "\n",
      "\n",
      "Topic 24 most common words: \n",
      "morning\n",
      "tenant\n",
      "friend\n",
      "missed\n",
      "date\n",
      "makes\n",
      "uber\n",
      "lease\n",
      "day\n",
      "eh\n",
      "leaving\n",
      "meeting\n",
      "texting\n",
      "understand\n",
      "scenario\n",
      "\n",
      "\n",
      "Topic 25 most common words: \n",
      "need\n",
      "late\n",
      "today\n",
      "forward\n",
      "gate\n",
      "week\n",
      "going\n",
      "pick\n",
      "talking\n",
      "started\n",
      "crime\n",
      "frame\n",
      "place\n",
      "fetch\n",
      "class\n",
      "\n",
      "\n",
      "Topic 26 most common words: \n",
      "home\n",
      "steve\n",
      "got\n",
      "start\n",
      "sushi\n",
      "working\n",
      "date\n",
      "rain\n",
      "send\n",
      "eh\n",
      "tomorrow\n",
      "late\n",
      "told\n",
      "key\n",
      "left\n",
      "\n",
      "\n",
      "Topic 27 most common words: \n",
      "morning\n",
      "tomorrow\n",
      "things\n",
      "excited\n",
      "good\n",
      "send\n",
      "repellent\n",
      "best\n",
      "meter\n",
      "way\n",
      "thinking\n",
      "gym\n",
      "says\n",
      "sweet\n",
      "water\n",
      "\n",
      "\n",
      "Topic 28 most common words: \n",
      "going\n",
      "nice\n",
      "worry\n",
      "like\n",
      "hello\n",
      "sorry\n",
      "talk\n",
      "lecture\n",
      "peanut\n",
      "tomorrow\n",
      "love\n",
      "work\n",
      "money\n",
      "tonight\n",
      "account\n",
      "\n",
      "\n",
      "Topic 29 most common words: \n",
      "think\n",
      "time\n",
      "mean\n",
      "took\n",
      "eh\n",
      "sleep\n",
      "day\n",
      "probably\n",
      "hatfield\n",
      "kay\n",
      "tight\n",
      "email\n",
      "desk\n",
      "water\n",
      "stuff\n",
      "\n",
      "\n",
      "Topic 30 most common words: \n",
      "come\n",
      "start\n",
      "wanna\n",
      "saying\n",
      "message\n",
      "alright\n",
      "kay\n",
      "key\n",
      "need\n",
      "thing\n",
      "text\n",
      "skipping\n",
      "edited\n",
      "wrong\n",
      "working\n",
      "\n",
      "\n",
      "Topic 31 most common words: \n",
      "think\n",
      "getting\n",
      "thank\n",
      "thought\n",
      "want\n",
      "insinuate\n",
      "help\n",
      "weekend\n",
      "year\n",
      "movie\n",
      "eh\n",
      "kea\n",
      "way\n",
      "long\n",
      "leave\n",
      "\n",
      "\n",
      "Topic 32 most common words: \n",
      "send\n",
      "getting\n",
      "like\n",
      "come\n",
      "saying\n",
      "outside\n",
      "wanna\n",
      "hope\n",
      "going\n",
      "talk\n",
      "peanut\n",
      "morning\n",
      "month\n",
      "coming\n",
      "scenario\n",
      "\n",
      "\n",
      "Topic 33 most common words: \n",
      "sushi\n",
      "today\n",
      "need\n",
      "coming\n",
      "like\n",
      "day\n",
      "let\n",
      "meet\n",
      "lecture\n",
      "says\n",
      "bring\n",
      "message\n",
      "lease\n",
      "study\n",
      "hope\n",
      "\n",
      "\n",
      "Topic 34 most common words: \n",
      "let\n",
      "lecture\n",
      "today\n",
      "come\n",
      "campus\n",
      "tired\n",
      "eh\n",
      "coming\n",
      "sent\n",
      "want\n",
      "probably\n",
      "open\n",
      "happened\n",
      "working\n",
      "meet\n",
      "\n",
      "\n",
      "Topic 35 most common words: \n",
      "time\n",
      "want\n",
      "friday\n",
      "send\n",
      "tlholo\n",
      "peanut\n",
      "coming\n",
      "way\n",
      "talking\n",
      "vn\n",
      "tomorrow\n",
      "tonight\n",
      "week\n",
      "probably\n",
      "leaving\n",
      "\n",
      "\n",
      "Topic 36 most common words: \n",
      "start\n",
      "let\n",
      "maybe\n",
      "straight\n",
      "find\n",
      "google\n",
      "went\n",
      "talking\n",
      "wants\n",
      "missed\n",
      "long\n",
      "makes\n",
      "morning\n",
      "cheat\n",
      "work\n",
      "\n",
      "\n",
      "Topic 37 most common words: \n",
      "wanted\n",
      "want\n",
      "check\n",
      "pick\n",
      "great\n",
      "partner\n",
      "nah\n",
      "saw\n",
      "morning\n",
      "friday\n",
      "leaving\n",
      "happening\n",
      "week\n",
      "rain\n",
      "wants\n",
      "\n",
      "\n",
      "Topic 38 most common words: \n",
      "talk\n",
      "friends\n",
      "cause\n",
      "sorry\n",
      "buy\n",
      "wanna\n",
      "missed\n",
      "voice\n",
      "friday\n",
      "thank\n",
      "meeting\n",
      "today\n",
      "insinuate\n",
      "decide\n",
      "come\n",
      "\n",
      "\n",
      "Topic 39 most common words: \n",
      "sorry\n",
      "lecture\n",
      "good\n",
      "send\n",
      "left\n",
      "working\n",
      "time\n",
      "sleep\n",
      "message\n",
      "playing\n",
      "month\n",
      "going\n",
      "edited\n",
      "update\n",
      "story\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "phi = nkw / nk.reshape(NUM_TOPICS, 1)\n",
    "\n",
    "num_words = 15\n",
    "for k in range(NUM_TOPICS):\n",
    "    most_common_words = np.argsort(phi[k])[::-1][:num_words]\n",
    "    print(f\"Topic {k} most common words: \")\n",
    "\n",
    "    for word in most_common_words:\n",
    "        print(vocab_idx_str[word])\n",
    "\n",
    "    print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
