{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import spacy\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "from spacy.lang.en import English\n",
    "import random\n",
    "import pandas as pd\n",
    "from tqdm import tqdm \n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('BetaData.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALPHA = 0.1\n",
    "BETA = 0.1\n",
    "NUM_TOPICS = 20\n",
    "sp = spacy.load('en_core_web_sm')\n",
    "\n",
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'full', 'one', '’m', 'nowhere', 'whom', 'was', 'although', 'whence', 'herein', 'made', 'are', 'among', 'such', 'whose', 'five', 'still', 'at', 'your', 'except', 'again', 'doing', 'everyone', 'say', 'keep', 'most', 'four', 'not', 'give', 'what', 'cannot', 'across', 'ourselves', 'much', '‘s', 'anywhere', 'himself', 'anyhow', 'bottom', 'already', 'their', 'he', 'being', 'ever', 'never', 'fifty', \"'re\", 'this', 'it', 'by', 'meanwhile', 'anything', 'has', '’ve', 'just', 'side', 'n‘t', 'against', 'themselves', 'yourself', 'n’t', 'became', 'none', 'her', \"'s\", 'everything', 'two', 'did', 'done', 'put', 'between', '’re', 'than', 'eight', 'few', 'off', 'beforehand', 'upon', 'becomes', 'of', 'moreover', 'make', 'down', 'yours', 'six', 'show', 'up', 'nor', 'yourselves', 'about', 'whether', 'seems', 'becoming', 'had', 'must', 'seem', 'she', 'do', 'they', 'around', 'further', 'too', 'after', 'the', 'more', \"'d\", \"'m\", 'while', 'am', 'itself', 'an', 'should', 'third', 'nevertheless', 're', 'well', 'elsewhere', 'become', 'therein', 'top', 'hereafter', 'noone', 'those', 'why', 'whole', 'onto', 'were', 'yet', 'part', 'anyone', 'that', 'me', 'some', 'per', 'enron', 'when', 'therefore', 'hence', 'mostly', 'back', 'have', 'seemed', 'whoever', 'its', 'here', 'own', 'toward', 'no', 'hers', 'rather', 'everywhere', 'latterly', 'either', '‘d', 'there', 'next', 'often', 'and', 'very', 'during', 'with', 'his', 'from', 'beyond', '‘m', 'where', 'anyway', 'get', 'twelve', '‘ve', 'them', 'ours', 'under', 'thence', 'each', 'i', 'whereas', 'sometimes', 'can', 'if', 'besides', 'many', 'along', 'whatever', 'now', 'eleven', 'a', 'thru', 'since', 'we', 'first', 'also', '‘re', 'all', 'hereby', 'someone', 'almost', 'amongst', 'nine', 'unless', '‘ll', 'in', 'above', 'both', 'mine', 'before', 'towards', 'will', 'may', 'see', 'wherever', 'how', 'others', 'various', 'would', 'other', 'least', 'herself', 'because', 'then', 'us', 'ca', 'indeed', 'whither', 'however', 'last', 'sixty', 'our', 'three', 'for', 'out', 'these', 'behind', 'once', 'same', 'even', 'serious', 'over', 'below', 'alone', 'using', 'thereby', 'on', 'twenty', 'but', 'is', 'through', 'him', 'used', \"'ve\", 'could', 'front', 'former', 'somehow', 'into', 'though', 'without', 'together', 'whereby', 'neither', 'until', 'which', 'thus', 'somewhere', 'every', 'sometime', 'namely', 'latter', 'or', 'quite', 'afterwards', 'so', 'beside', 'move', 'call', 'ten', 'nothing', 'fifteen', 'due', '’d', 'go', '’s', 'seeming', 'less', 'myself', 'always', \"n't\", 'only', 'name', 'otherwise', 'whereafter', 'formerly', 'hundred', 'hereupon', 'might', 'thereafter', 'you', 'does', 'really', 'regarding', 'something', 'wherein', 'my', 'within', 'to', 'else', 'any', 'forty', 'enough', 'another', \"'ll\", 'been', 'via', 'amount', 'please', 'empty', 'be', 'throughout', 'who', 'whenever', 'several', '’ll', 'take', 'perhaps', 'nobody', 'thereupon', 'whereupon', 'as'}\n"
     ]
    }
   ],
   "source": [
    "print(sp.Defaults.stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_frequencies(data, max_docs = 10000):\n",
    "    freqs = Counter()\n",
    "    all_stopwords = sp.Defaults.stop_words\n",
    "    all_stopwords.add(\"enron\")\n",
    "    nr_tokens = 0\n",
    "\n",
    "    for doc in data[:max_docs]:\n",
    "        if isinstance(doc, str):\n",
    "            tokens = sp.tokenizer(doc)\n",
    "            for token in tokens:\n",
    "                token_text = token.text.lower()\n",
    "                if token_text not in all_stopwords and token.is_alpha:\n",
    "                    nr_tokens += 1\n",
    "                    freqs[token_text] += 1\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    return freqs\n",
    "\n",
    "def get_vocab(freqs, freq_threshold=3):\n",
    "    vocab = {}\n",
    "    vocab_idx_str = {}\n",
    "    vocab_idx = 0\n",
    "\n",
    "    for word in freqs:\n",
    "        if freqs[word] >= freq_threshold:\n",
    "            vocab[word]= vocab_idx\n",
    "            vocab_idx_str[vocab_idx] = word\n",
    "            vocab_idx += 1\n",
    "    return vocab, vocab_idx_str\n",
    "        \n",
    "\n",
    "\n",
    "def tokenize_dataset(data, vocab, max_docs= 10000):\n",
    "    nr_tokens = 0\n",
    "    nr_docs = 0\n",
    "    docs = []\n",
    "\n",
    "    for doc in data[:max_docs]:\n",
    "        tokens = sp.tokenizer(doc)\n",
    "\n",
    "        if len(tokens) > 1:\n",
    "            doc = []\n",
    "            for token in tokens:\n",
    "                token_text = token.text.lower()\n",
    "                if token_text in vocab:\n",
    "                    doc.append(token_text)\n",
    "                    nr_tokens += 1\n",
    "            nr_docs += 1\n",
    "            docs.append(doc)\n",
    "\n",
    "    print(f\"Number of text messages: {nr_docs}\")\n",
    "    print(f\"Number of tokens: {nr_tokens}\")\n",
    "\n",
    "    corpus = []\n",
    "    for doc in docs:\n",
    "        corpus_d = []\n",
    "\n",
    "        for token in doc:\n",
    "            corpus_d.append(vocab[token])\n",
    "\n",
    "        corpus.append(np.asarray(corpus_d))\n",
    "\n",
    "    return docs, corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of text messages: 1425\n",
      "Number of tokens: 1791\n",
      "Vocab size: 246\n"
     ]
    }
   ],
   "source": [
    "data = df['Column1'].astype(str).sample(frac=1).values\n",
    "freqs = generate_frequencies(data)\n",
    "vocab, vocab_idx_str = get_vocab(freqs)\n",
    "docs, corpus = tokenize_dataset(data, vocab)\n",
    "vocab_size = len(vocab)\n",
    "print(f\"Vocab size: {vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:02<00:00, 88.62it/s]\n"
     ]
    }
   ],
   "source": [
    "def LDA_Collapsed_Gibbs(corpus, num_iter=200):\n",
    "\n",
    "    Z = []\n",
    "    num_docs = len(corpus)\n",
    "\n",
    "    for _, doc in enumerate(corpus):\n",
    "        Zd = np.random.randint(low=0, high = NUM_TOPICS, size=(len(doc)))\n",
    "        Z.append(Zd)\n",
    "\n",
    "    ndk = np.zeros((num_docs, NUM_TOPICS))\n",
    "    for d in range(num_docs):\n",
    "        for k in range(NUM_TOPICS):\n",
    "            ndk[d, k] = np.sum(Z[d]==k)\n",
    "\n",
    "    nkw = np.zeros((NUM_TOPICS, vocab_size))\n",
    "    for doc_idx, doc in enumerate(corpus):\n",
    "        for i, word in enumerate(doc):\n",
    "            topic = Z[doc_idx][i]\n",
    "            nkw[topic, word] += 1\n",
    "\n",
    "    nk = np.sum(nkw, axis=1)\n",
    "    topic_list = [i for i in range(NUM_TOPICS)]\n",
    "\n",
    "    for _ in tqdm(range(num_iter)):\n",
    "        for doc_idx, doc in enumerate(corpus):\n",
    "            for i in range(len(doc)):\n",
    "                word = doc[i]\n",
    "                topic = Z[doc_idx][i]\n",
    "\n",
    "                ndk[doc_idx, topic] -= 1\n",
    "                nkw[topic, word] -= 1\n",
    "                nk[topic] -= 1\n",
    "\n",
    "                p_z = (ndk[doc_idx, :] + ALPHA * (nkw[:, word] + BETA)/ (nk[:] + BETA*vocab_size))\n",
    "                topic = random.choices(topic_list, weights=p_z, k=1)[0]\n",
    "\n",
    "                Z[doc_idx][i] = topic\n",
    "                ndk[doc_idx, topic] += 1\n",
    "                nkw[topic, word] += 1\n",
    "                nk[topic] += 1\n",
    "\n",
    "    return Z, ndk, nkw, nk\n",
    "\n",
    "Z, ndk, nkw, nk = LDA_Collapsed_Gibbs(corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0 most common words: \n",
      "null\n",
      "hey\n",
      "vn\n",
      "girl\n",
      "pay\n",
      "na\n",
      "gon\n",
      "leaving\n",
      "tomorrow\n",
      "yep\n",
      "listen\n",
      "soon\n",
      "free\n",
      "tell\n",
      "yoh\n",
      "\n",
      "\n",
      "Topic 1 most common words: \n",
      "okay\n",
      "know\n",
      "let\n",
      "day\n",
      "ready\n",
      "coming\n",
      "null\n",
      "yoh\n",
      "sushi\n",
      "bed\n",
      "taking\n",
      "working\n",
      "week\n",
      "start\n",
      "look\n",
      "\n",
      "\n",
      "Topic 2 most common words: \n",
      "know\n",
      "let\n",
      "today\n",
      "come\n",
      "lecture\n",
      "ready\n",
      "want\n",
      "finished\n",
      "night\n",
      "uber\n",
      "kay\n",
      "tell\n",
      "kea\n",
      "break\n",
      "book\n",
      "\n",
      "\n",
      "Topic 3 most common words: \n",
      "null\n",
      "sure\n",
      "yeah\n",
      "morning\n",
      "like\n",
      "moment\n",
      "asking\n",
      "check\n",
      "come\n",
      "quick\n",
      "voice\n",
      "missed\n",
      "hear\n",
      "hi\n",
      "paid\n",
      "\n",
      "\n",
      "Topic 4 most common words: \n",
      "come\n",
      "try\n",
      "morning\n",
      "going\n",
      "null\n",
      "sweet\n",
      "group\n",
      "pick\n",
      "man\n",
      "hours\n",
      "days\n",
      "know\n",
      "cool\n",
      "outside\n",
      "wait\n",
      "\n",
      "\n",
      "Topic 5 most common words: \n",
      "know\n",
      "let\n",
      "sorry\n",
      "working\n",
      "better\n",
      "girl\n",
      "awe\n",
      "buy\n",
      "place\n",
      "thought\n",
      "tenant\n",
      "arrive\n",
      "like\n",
      "today\n",
      "hey\n",
      "\n",
      "\n",
      "Topic 6 most common words: \n",
      "said\n",
      "missed\n",
      "voice\n",
      "like\n",
      "think\n",
      "yup\n",
      "sorry\n",
      "yes\n",
      "year\n",
      "time\n",
      "asked\n",
      "friend\n",
      "thank\n",
      "office\n",
      "campus\n",
      "\n",
      "\n",
      "Topic 7 most common words: \n",
      "yes\n",
      "good\n",
      "home\n",
      "talk\n",
      "yeah\n",
      "okidoki\n",
      "time\n",
      "find\n",
      "tlholo\n",
      "friend\n",
      "let\n",
      "need\n",
      "know\n",
      "hana\n",
      "ask\n",
      "\n",
      "\n",
      "Topic 8 most common words: \n",
      "oh\n",
      "sure\n",
      "yeah\n",
      "work\n",
      "month\n",
      "happening\n",
      "going\n",
      "hatfield\n",
      "said\n",
      "right\n",
      "way\n",
      "lecture\n",
      "hello\n",
      "carry\n",
      "moving\n",
      "\n",
      "\n",
      "Topic 9 most common words: \n",
      "null\n",
      "hey\n",
      "wanna\n",
      "oh\n",
      "hatfield\n",
      "nice\n",
      "girl\n",
      "year\n",
      "late\n",
      "tomorrow\n",
      "cheat\n",
      "chat\n",
      "come\n",
      "today\n",
      "hope\n",
      "\n",
      "\n",
      "Topic 10 most common words: \n",
      "yep\n",
      "happened\n",
      "delivery\n",
      "kay\n",
      "sending\n",
      "wait\n",
      "gon\n",
      "na\n",
      "mind\n",
      "need\n",
      "talking\n",
      "eh\n",
      "asked\n",
      "alright\n",
      "repellent\n",
      "\n",
      "\n",
      "Topic 11 most common words: \n",
      "morning\n",
      "okay\n",
      "time\n",
      "gon\n",
      "na\n",
      "send\n",
      "meet\n",
      "bring\n",
      "getting\n",
      "tonight\n",
      "tomorrow\n",
      "tlholo\n",
      "sleep\n",
      "t\n",
      "come\n",
      "\n",
      "\n",
      "Topic 12 most common words: \n",
      "yeah\n",
      "eh\n",
      "going\n",
      "okay\n",
      "hope\n",
      "stuff\n",
      "open\n",
      "time\n",
      "yes\n",
      "happened\n",
      "start\n",
      "murder\n",
      "outside\n",
      "ask\n",
      "home\n",
      "\n",
      "\n",
      "Topic 13 most common words: \n",
      "yeah\n",
      "need\n",
      "come\n",
      "thank\n",
      "monday\n",
      "alright\n",
      "yes\n",
      "skipping\n",
      "desk\n",
      "thanks\n",
      "tomorrow\n",
      "find\n",
      "sure\n",
      "data\n",
      "help\n",
      "\n",
      "\n",
      "Topic 14 most common words: \n",
      "hey\n",
      "yes\n",
      "time\n",
      "know\n",
      "home\n",
      "think\n",
      "best\n",
      "come\n",
      "going\n",
      "right\n",
      "page\n",
      "na\n",
      "mind\n",
      "rain\n",
      "let\n",
      "\n",
      "\n",
      "Topic 15 most common words: \n",
      "know\n",
      "let\n",
      "whoop\n",
      "forward\n",
      "way\n",
      "money\n",
      "want\n",
      "thinking\n",
      "yes\n",
      "like\n",
      "study\n",
      "year\n",
      "told\n",
      "step\n",
      "apartment\n",
      "\n",
      "\n",
      "Topic 16 most common words: \n",
      "got\n",
      "going\n",
      "https\n",
      "null\n",
      "day\n",
      "weekend\n",
      "think\n",
      "sleep\n",
      "wants\n",
      "thought\n",
      "good\n",
      "want\n",
      "sold\n",
      "partner\n",
      "mean\n",
      "\n",
      "\n",
      "Topic 17 most common words: \n",
      "coming\n",
      "hey\n",
      "gym\n",
      "message\n",
      "girl\n",
      "tomorrow\n",
      "maybe\n",
      "tight\n",
      "tell\n",
      "sleep\n",
      "https\n",
      "meet\n",
      "want\n",
      "great\n",
      "null\n",
      "\n",
      "\n",
      "Topic 18 most common words: \n",
      "null\n",
      "yoh\n",
      "today\n",
      "feeling\n",
      "friday\n",
      "sweet\n",
      "thinking\n",
      "cause\n",
      "hey\n",
      "things\n",
      "oh\n",
      "night\n",
      "apartment\n",
      "cheat\n",
      "excited\n",
      "\n",
      "\n",
      "Topic 19 most common words: \n",
      "meeting\n",
      "start\n",
      "yeah\n",
      "okay\n",
      "got\n",
      "ai\n",
      "hi\n",
      "place\n",
      "heyo\n",
      "data\n",
      "oh\n",
      "nighter\n",
      "alright\n",
      "need\n",
      "working\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "phi = nkw / nk.reshape(NUM_TOPICS, 1)\n",
    "\n",
    "num_words = 15\n",
    "for k in range(NUM_TOPICS):\n",
    "    most_common_words = np.argsort(phi[k])[::-1][:num_words]\n",
    "    print(f\"Topic {k} most common words: \")\n",
    "\n",
    "    for word in most_common_words:\n",
    "        print(vocab_idx_str[word])\n",
    "\n",
    "    print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
